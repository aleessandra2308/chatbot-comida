# -*- coding: utf-8 -*-
"""Practica_3_E_commerce_Parte_2 (DESARROLLADO EN CLASE).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EHVTkDyD4XmkBDV0Chcy7MNhheY03M5h

###Curso: Procesamiento del Lenguaje Natural (PLN) para Chatbots
####Adaptado por:


####ING. MSC. WILDER NINA CHOQUEHUAYTA
####BACH. NADIA LIZ QUISPE SIANCAS

# E-commerce/chatbot de producto PARTE 2

Este notebook implementa un asistente virtual de WhatsApp para Ninatec, una empresa de tecnología. Utiliza:

*   Firebase para obtener los productos y sus precios desde Firestore.
*   Twilio para enviar y recibir mensajes vía WhatsApp.
*   Un modelo LLaMA fine-tuning  para responder preguntas generales.

*   Flask para levantar un servidor web que recibe y responde mensajes en tiempo real.


El asistente reconoce preguntas sobre precios, identifica productos mencionados, consulta la base de datos y responde automáticamente con el precio. Si no es una pregunta sobre precios, genera una respuesta usando IA.

## Fine-Tuning de un Modelo de Lenguaje para E-commerce
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Prompt:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token
def formatting_prompts_func(examples):
    inputs       = examples["prompt"]
    outputs      = examples["response"]
    texts = []
    for input, output in zip(inputs, outputs):
        text = alpaca_prompt.format(input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

from datasets import load_dataset
username = "NadiaLiz"
dataset = load_dataset(username + "/ninatec_dataset", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)

trainer_stats = trainer.train()

model.save_pretrained("Llama-3.2.3B")
tokenizer.save_pretrained("Llama-3.2.3B")

"""## Instalar Librerías

*   **firebase-admin:** Acceso a Firebase/Firestore.
*   **twilio:** Enviar y recibir mensajes por WhatsApp.
*   **pyngrok:** Exponer el servidor Flask con un túnel.
*   **transformers:** Cargar modelos de lenguaje.
*   **flask:** Crear la API web.







"""

!pip install firebase-admin twilio transformers pyngrok flask

"""## Firebase

*   Carga las credenciales de Firebase desde un archivo JSON.
*   Obtiene el cliente Firestore (db) para interactuar con la base de datos.


"""

import firebase_admin
from firebase_admin import credentials, firestore

if not firebase_admin._apps:
    cred = credentials.Certificate("/content/ninatec-e026f-firebase-adminsdk-fbsvc-1fcb5b745f.json")
    firebase_admin.initialize_app(cred)

db = firestore.client() # conexion a la base de datos

"""## Twilio

*   Se configuran las credenciales de Twilio.
*   Se crea el cliente para enviar mensajes de WhatsApp.


"""

from twilio.rest import Client

TWILIO_ACCOUNT_SID = "AC5b4967b079377659dd12f50cdde59f5b"
TWILIO_AUTH_TOKEN = "8dba2785bfe130e282e8e8fdc6627ea6"
TWILIO_WHATSAPP_NUMBER = "whatsapp:+14155238886"

client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)

"""## Cargar Modelo

*   Carga tu modelo LLaMA fine-tuning desde la ruta local.
*   Usa 4-bit quantization para optimizar el rendimiento.
*   Prepara el modelo para generar respuestas (inferencia).




"""

from unsloth import FastLanguageModel

model_path = "./Llama-3.2.3B"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_path,
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)
model = FastLanguageModel.for_inference(model)

"""## Ngrok


*   Ngrok genera una URL pública accesible


"""

!pkill ngrok
!ngrok authtoken 23jTF3I3jpQEZIOQrNMJZex6n0z_7zZq2FrBtx9iJByAFrW2b
from pyngrok import ngrok
public_url = ngrok.connect(5000, proto="http")
print("Ngrok URL pública:", public_url)

"""Este conjunto de funciones permite al asistente virtual de Ninatec identificar productos mencionados en los mensajes de los usuarios y responder con el precio correspondiente utilizando datos almacenados en Firebase.


*   **obtener_contexto():** Define el rol del asistente.
*   **normalizar(texto):** Convierte texto a minúsculas y elimina acentos para facilitar comparaciones.
*   **extraer_producto(mensaje):** Busca en el mensaje del usuario palabras que coincidan con productos de la base de datos.
*   **buscar_precio(producto):** Consulta Firestore para obtener el precio del producto identificado y responde con un mensaje claro.

"""

import unicodedata

def obtener_contexto():
    return (
        "Eres un asistente virtual de NinaFood, un restaurante especializado en comida peruana. "
        "Estás capacitado para responder preguntas sobre nuestros productos, delivery y medios de pago. "
        "Responde siempre de manera amable, clara y profesional."
    )

def normalizar(texto):
    texto = texto.lower()
    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')
    return texto

def extraer_producto(mensaje):
    mensaje_norm = normalizar(mensaje)
    productos_ref = db.collection("Productos").stream()

    for doc in productos_ref:
        nombre_producto = doc.to_dict().get("producto")
        nombre_norm = normalizar(nombre_producto)

        if nombre_norm in mensaje_norm:
            return doc.id
    return None

def buscar_precio(producto):
    ref = db.collection("Productos").document(producto)
    doc = ref.get()

    if doc.exists:
        data = doc.to_dict()
        producto = data.get("producto")
        precio = data.get("precio")

        return f"Claro, el {producto} cuesta S/{precio}."
    else:
        return None

"""## Endpoint
Se define el endpoint /whatsapp para recibir mensajes de WhatsApp usando Flask y Twilio.
Este código permite que el chatbot converse de forma inteligente y responda tanto preguntas específicas como generales.
"""

from flask import Flask, request
from twilio.twiml.messaging_response import MessagingResponse

app = Flask(__name__)

@app.route('/whatsapp', methods=['POST'])
def whatsapp_mymessage():
    incoming_msg = request.values.get('Body', '').lower()
    print("Mensaje recibido:", incoming_msg)
    from_number = request.values.get('From', '')

    respuesta = ""
    contexto = obtener_contexto()

    if "precio" in incoming_msg or "cuánto cuesta" in incoming_msg:
        producto = extraer_producto(incoming_msg)
        if producto:
            respuesta = buscar_precio(producto)
            if not respuesta:
                respuesta = f"Lo siento, no encontré detalles para el producto '{producto}'."
        else:
            respuesta = "¿De qué plato deseas más información?"
    else:

        prompt = (
            f"{contexto}\n\n"
            f"Usuario: {incoming_msg}\n"
            f"Asistente:"
        )

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)
        respuesta_generada = tokenizer.decode(outputs[0], skip_special_tokens=True)
        respuesta = respuesta_generada.replace(prompt, "").strip()
    print("Respuesta generada:", respuesta)
    return f"<Response><Message>{respuesta}</Message></Response>"

"""Inicia el servidor Flask local en el puerto 5000 y lo hace accesible desde cualquier IP, necesario para que Ngrok lo exponga públicamente y pueda recibir mensajes de Twilio."""

if __name__ == '__main__':
    app.run(port=5000)

"""# Ejemplo de conversación
### > Hola
### > que productos vendes
### > cual  es el precio del cable usb?
### > cual es el medio de pago?

## Tarea 3: Flujo conversacional con Fine-Tuning de Llama 3.2 con venta de Comida Peruana

**Instrucciones:**

Replicar el notebook con sus tokes y datos con su modelo de fine-tuning de venta de comida peruana y que pueda conversal via whatsapp.


**Envío:**
El envio es grupal (5 personas) por canvas. Solo un representante del grupo envía y los otros se agregan al grupo en la sección de personas.

**Presentación:**
Realizar un informe .pdf de lo trabajado de las instrucciones dadas. El informe obtendrá capturas de código y funcionamiento del mismo.
"""